import torch
from diffusers import StableDiffusionPipeline
import os
import sys
from datetime import datetime
import argparse
import re
import google.generativeai as genai
from dotenv import load_dotenv
import json

def generate_enhanced_prompts(user_prompt: str, gemini_model) -> tuple[str, str]:
    """
    Geminiã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç°¡å˜ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è©³ç´°ãªè‹±èªã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¤‰æ›ã—ã¾ã™ã€‚

    Args:
        user_prompt (str): ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‚
        gemini_model: åˆæœŸåŒ–æ¸ˆã¿ã®Geminiãƒ¢ãƒ‡ãƒ«ã€‚

    Returns:
        tuple[str, str]: (é«˜å“è³ªåŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ, ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ)
    """
    print("ğŸ¤– GeminiãŒæœ€é©ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è€ƒãˆã¦ã„ã¾ã™...")
    try:
        instruction = f"""You are an expert prompt engineer for a photorealistic image generation AI. Your task is to take the user's core subject and enhance it into a detailed, high-quality English prompt.

**Crucially, the final prompt must be a direct and faithful representation of the user's core subject.** Do not add unrelated elements or change the main subject. For example, if the user asks for "sunflower", create a prompt for a beautiful sunflower, not a painting of a sunflower by Van Gogh unless they specifically ask for it.

Also, provide a corresponding negative prompt to avoid common image generation issues.

User's Core Subject: ã€Œ{user_prompt}ã€

Provide the output in a JSON format with two keys: "prompt" and "negative_prompt".
Example for "sunflower": {{"prompt": "A vibrant, photorealistic sunflower in full bloom, facing the sun, with detailed yellow petals and a dark, seed-filled center. 4K, high detail, sharp focus.", "negative_prompt": "cartoon, painting, ugly, deformed, blurry, low quality, multiple flowers"}}
"""
        response = gemini_model.generate_content(instruction)
        # AIãŒè¿”ã™å¯èƒ½æ€§ã®ã‚ã‚‹ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰Šé™¤ã—ã¦å®‰å®šæ€§ã‚’å‘ä¸Š
        cleaned_text = response.text.strip().replace("```json", "").replace("```", "")
        prompts = json.loads(cleaned_text)
        print(f"âœ¨ ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompts['prompt']}")
        print(f"ğŸš« ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompts['negative_prompt']}")
        return prompts['prompt'], prompts['negative_prompt']
    except Exception as e:
        print(f"âš ï¸ Geminiã§ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", file=sys.stderr)
        print("å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ç”»åƒã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã‚‚ã€å°‘ã—ã§ã‚‚è‰¯ã„çµæœãŒå‡ºã‚‹ã‚ˆã†ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿½åŠ 
        return f"photorealistic, high quality, {user_prompt}", "ugly, deformed, blurry, low quality, cartoon, anime"

def generate_and_save_image(pipe, prompt: str, negative_prompt: str, save_folder: str):
    """
    ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®Stable Diffusionãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‚’ç”Ÿæˆã—ã€æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã—ã¾ã™ã€‚

    Args:
        pipe: ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®StableDiffusionPipelineã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
        prompt (str): ç”»åƒç”Ÿæˆã®ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‚
        negative_prompt (str): ç”Ÿæˆã—ã¦ã»ã—ããªã„è¦ç´ ã‚’æŒ‡å®šã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‚
        save_folder (str): ç”»åƒã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹ã€‚
    """
    try:
        print(f"ğŸš« ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: '{negative_prompt}'")
        # --- 1. ç”»åƒç”Ÿæˆ ---
        print(f"ğŸ¨ '{prompt}' ã®ç”»åƒã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™...")
        # `pipe`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ¸¡ã™ã ã‘ã§ç”»åƒãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚
        # ç”Ÿæˆã•ã‚ŒãŸç”»åƒã¯PIL.Imageã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã™ã€‚
        image = pipe(prompt, negative_prompt=negative_prompt).images[0]

        # --- 2. ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆã—ã¦ç”»åƒã‚’ä¿å­˜ ---
        os.makedirs(save_folder, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«åã¨ã—ã¦ä½¿ãˆã‚‹ã‚ˆã†ã«ã‚µãƒ‹ã‚¿ã‚¤ã‚º
        prompt_slug = re.sub(r'[\\/*?:"<>|]', "", prompt)[:50] # ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ãˆãªã„æ–‡å­—ã‚’å‰Šé™¤ã—ã€é•·ã•ã‚’åˆ¶é™
        filename = f"{timestamp}_{prompt_slug}.png"
        save_path = os.path.join(save_folder, filename)

        print(f"ğŸ’¾ ç”»åƒã‚’ä¿å­˜ä¸­... -> {save_path}")
        image.save(save_path)

        print(f"âœ… ç”»åƒã‚’ä¿å­˜ã—ã¾ã—ãŸ: {save_path}")

    except Exception as e:
        print(f"âŒ äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", file=sys.stderr)

def main():
    """ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    load_dotenv()
    parser = argparse.ArgumentParser(
        description="Stable Diffusionã‚’ä½¿ã£ã¦å¯¾è©±å½¢å¼ã§ç”»åƒã‚’ç”Ÿæˆã—ã€ä¿å­˜ã—ã¾ã™ã€‚",
        formatter_class=argparse.RawTextHelpFormatter # ãƒ˜ãƒ«ãƒ—ã®æ”¹è¡Œã‚’ç¶­æŒ
    )
    parser.add_argument(
        "-o", "--output",
        type=str,
        default="img",
        help="ç”»åƒã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹ã€‚ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: img)"
    )
    parser.add_argument(
        "--model_id",
        type=str,
        default="stabilityai/stable-diffusion-2-1-base",
        help="ä½¿ç”¨ã™ã‚‹Stable Diffusionãƒ¢ãƒ‡ãƒ«ã®IDã€‚ (ä¾‹: 'stabilityai/stable-diffusion-xl-base-1.0')"
    )
    args = parser.parse_args()

    try:
        # --- Geminiãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å‡¦ç† ---
        gemini_model = None
        api_key = os.getenv("GOOGLE_API_KEY")
        if api_key:
            try:
                genai.configure(api_key=api_key)
                gemini_model = genai.GenerativeModel('gemini-1.0-pro')
                print("âœ… Geminiãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ³ãŒæœ‰åŠ¹ã§ã™ã€‚")
            except Exception as e:
                print(f"âš ï¸ Geminiã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", file=sys.stderr)
                print("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è‡ªå‹•ç”Ÿæˆã¯ç„¡åŠ¹ã«ãªã‚Šã¾ã™ã€‚")
        else:
            print("â„¹ï¸ GOOGLE_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è‡ªå‹•ç”Ÿæˆã¯ç„¡åŠ¹ã§ã™ã€‚")

        # --- ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å‡¦ç† (ä¸€åº¦ã ã‘å®Ÿè¡Œ) ---
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"â„¹ï¸ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
        if device == "cpu":
            print("âš ï¸ è­¦å‘Š: CPUã§å®Ÿè¡Œã—ã¦ã„ã¾ã™ã€‚ç”»åƒã®ç”Ÿæˆã«ã¯æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚")

        print(f"ğŸ”„ ãƒ¢ãƒ‡ãƒ« '{args.model_id}' ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™... (åˆå›ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™)")
        torch_dtype = torch.float16 if device == "cuda" else torch.float32
        pipe = StableDiffusionPipeline.from_pretrained(args.model_id, torch_dtype=torch_dtype)
        pipe = pipe.to(device)
        print("âœ… ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

        # --- å¯¾è©±ãƒ«ãƒ¼ãƒ— ---
        print("\nğŸ¨ ç”»åƒç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        print("ï¼ˆçµ‚äº†ã™ã‚‹ã«ã¯ 'quit' ã¾ãŸã¯ 'exit' ã¨å…¥åŠ›ï¼‰")
        while True:
            user_prompt = input("âœ… ä½•ã‚’ç”Ÿæˆã—ã¾ã™ã‹ï¼Ÿ > ").strip()
            if user_prompt.lower() in ["quit", "exit"]:
                print("ğŸ‘‹ çµ‚äº†ã—ã¾ã™ã€‚")
                break
            if user_prompt:
                # GeminiãŒä½¿ãˆãªã„å ´åˆã‚„å¤±æ•—ã—ãŸå ´åˆã®ã€å …å®Ÿãªãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
                prompt = f"photorealistic, high quality, {user_prompt}"
                negative_prompt = "ugly, deformed, blurry, low quality, cartoon, anime, text, watermark"
                # GeminiãŒåˆ©ç”¨å¯èƒ½ãªã‚‰ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é«˜å“è³ªåŒ–ã™ã‚‹
                if gemini_model:
                    prompt, negative_prompt = generate_enhanced_prompts(user_prompt, gemini_model)
                generate_and_save_image(pipe, prompt, negative_prompt, args.output)
                print("-" * 20) # åŒºåˆ‡ã‚Šç·š
    except Exception as e:
        print(f"âŒ è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", file=sys.stderr)

if __name__ == "__main__":
    main()
